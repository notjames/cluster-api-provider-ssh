
          set -e
          set -x
          (
          ARCH=amd64

          # Need to have common_functions sourced first!
          if [[ -z "${MASTER_IP}" ]];then
            echo >&2 "Cannot continue. \$MASTER_IP is not set."
            exit 50
          fi

          if [[ -z "${CONTROL_PLANE_VERSION}" ]];then
            echo >&2 "Cannot continue. \$CONTROL_PLANE_VERSION is not set."
            exit 51
          fi

          if [[ -z "${POD_CIDR}" ]];then
            echo >&2 "Cannot continue. \$POD_CIDR is not set."
            exit 53
          fi

          if [[ -z "${SERVICE_CIDR}" ]];then
            echo >&2 "Cannot continue. \$SERVICE_CIDR is not set."
            exit 55
          fi

          # range is bounded by network (-n) & broadcast (-b) addresses.
          # the following uses `read` with a here-statement to assign the output of
          # ipcalc -bn into two variables; $hi and $lo the output of which is cut and then
          # delimited by a ":". Read uses $IFS to automatically split on that delimiter.
          IFS=':' read -r hi lo <<< "$(ipcalc -bn "${SERVICE_CIDR}" | cut -f 2 -d = | sed -r 'N;s/\n/:/')"

          # similar to above only this is splitting on '.'.
          IFS='.' read -r a b c d <<< "$lo"
          IFS='.' read -r e f g h <<< "$hi"

          # kubeadm uses 10th IP as DNS server
          CLUSTER_DNS_SERVER="$(eval "echo {$a..$e}.{$b..$f}.{$c..$g}.{$d..$h}" | awk '{print $11}')"
          export CLUSTER_DNS_SERVER

          # chk and fix /etc/hosts
          hosts=(quay.io gcr.io k8s.gcr.io registry-1.docker.io docker.io packages.cloud.google.com)
          nexus_host="182.195.81.113"
          budate="$(date +%Y%m%dT%H%M%S)"
          bu_etc_hosts_file="/etc/hosts-$budate"

          # if we can ping google then we're probably not in the lab
          # and this fixup is not necessary.
          if ! ping -c 1 8.8.4.4 >/dev/null 2>&1; then
            current_entry=$(grep -Po "$nexus_host.*" /etc/hosts)

            if grep -q $nexus_host /etc/hosts; then
              if cp /etc/hosts $bu_etc_hosts_file; then
                if ! fix_etc_hosts; then
                  echo >&2 "Unable to edit-in-place /etc/hosts."
                fi
              else
                echo >&2 "Unable to copy /etc/hosts to $bu_etc_hosts_file"
                exit 20
              fi
            else
              local add new_entry

              new_entry="$current_entry"
              add=0

              for h in ${hosts[@]}; do
                if ! $(echo "$entry" | grep -q "$h"); then
                  ((add++))
                  new_entry+=" $h"
                fi
              done

              if [[ $add ]]; then
                if ! sed -r -i "s/^($current_entry)/#\1/" /etc/hosts; then
                  # put the original back
                  mv $bu_etc_hosts_file /etc/hosts
                  exit 21
                fi

                echo "$new_entry" >> /etc/hosts
              fi
            fi
          fi

          docker_service='/usr/lib/systemd/system/docker.service'

          # install_docker
          # Our current kubeadm doesn't work right with docker-ce 18.3 provided
          # by our currently used AMI. Also, we want to know we're getting the
          # stock provided docker for the system on the pod everytime. So we'll just
          # remove and reinstall docker every time we install.
          sudo yum remove -y $(rpm -qa 'docker*')
          sudo yum install -y docker
          sudo systemctl daemon-reload && sudo systemctl start docker.service

          # enable insecure image registry
          sudo mkdir -p /etc/docker
          sudo cp /dev/stdin /etc/docker/daemon.json  <<< '
          {
              "insecure-registries": [
                  "docker",
                  "docker.io",
                  "registry-1.docker.io",
                  "gcr.io",
                  "k8s.gcr.io",
                  "quay.io",
                  "182.195.81.113:9401",
                  "182.195.81.113:9402",
                  "182.195.81.113:9403",
                  "182.195.81.113:9404"
              ]
          }'

          if [[ ! -f "$docker_service" ]]; then
            echo >&2 'Cannot update docker.service file. "$docker_service" does not exist.'
            exit 30
          fi

          if [[ $(grep -c "native.cgroupdriver=systemd" "$docker_service" 2>/dev/null) == 0 ]]; then
            if ! sudo sed -r -i 's#^(ExecStart=/usr/bin/dockerd)#\1 --exec-opt native.cgroupdriver=systemd --exec-opt runtime-cgroups=/systemd/system.slice --exec-opt kubelet-cgroups=/systemd/system.slice --exec-opt MountFlags=private#' \
                 "$docker_service"; then
              echo >&2 "Unable to update '$docker_service' with proper cgroupdriver."
              exit 31
            fi
          else
            echo >&2 "WARNING: Looks like '$docker_service' was already updated. Skipping."
          fi

          if sudo cp /dev/stdin /etc/sysconfig/docker <<< 'DOCKER_OPTS="--iptables=false --ip-masq=false"'; then
            [[ -z ${USER+x} ]] && USER=$(whoami)
            sudo usermod -a -G docker $USER
            sudo chmod 640 /etc/sysconfig/docker
          else
            echo >&2 "Unable to update /etc/sysconfig/docker."
            exit 32
          fi

          if ! sudo systemctl enable --now docker;then
            echo >&2 "Unable to 'systemctl enable docker'. Quitting."
            exit 33
          fi

          if ! sudo systemctl daemon-reload; then
            echo >&2 "Unable to reload systemctl daemon."
            exit 34
          fi

          if sudo systemctl restart docker.service; then
            echo "docker is installed successfully."
          else
            echo >&2 "systemctl restart docker failed."
            exit 35
          fi
          # end install docker

          # install k8s
          if [[ -z $KUBELET_VERSION ]]; then
            echo >&2 "FATAL: \$KUBELET_VERSION is nil! Cannot continue."
            exit 40
          fi

          sudo cp /dev/stdin /etc/yum.repos.d/kubernetes.repo <<< '
          [kubernetes]
          name=Kubernetes
          baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
          enabled=1
          gpgcheck=1
          repo_gpgcheck=1
          gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg' && \

          sudo sed -r -i 's#^\ +##g' /etc/yum.repos.d/kubernetes.repo

          # Set SELinux in permissive mode (effectively disabling it)
          sudo setenforce 0
          sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

          sudo yum install -y "kubelet-${KUBELET_VERSION}" \
                              "kubeadm-${KUBELET_VERSION}" \
                              "kubectl-${KUBELET_VERSION}" \
                              conntrack --disableexcludes=kubernetes

          # See https://samsung-cnct.atlassian.net/browse/CMS-391
          # If the file exists, grok it first (preserving current settings)
          if [[ -d /var/lib/kubelet ]]; then
            if [[ -f /var/lib/kubelet/kubeadm-flags.env ]]; then
              source /var/lib/kubelet/kubeadm-flags.env

              # change the one we want to change
              sudo echo "KUBELET_KUBEADM_ARGS=--cgroup-driver=systemd" >> /var/lib/kubelet/kubeadm-flags.env
            else
                sudo cp /dev/stdin /var/lib/kubelet/kubeadm-flags.env <<< \
                "KUBELET_KUBEADM_ARGS=--cgroup-driver=systemd"
            fi
          fi

          sudo systemctl enable kubelet && sudo systemctl start kubelet
          # end install k8s


          # configure_kubelet_systemd
            sudo cp /dev/stdin /etc/systemd/system/kubelet.service.d/20-kubelet.conf <<< "[Service]
          Environment='KUBELET_DNS_ARGS=--cluster-dns=${CLUSTER_DNS_SERVER} --cluster-domain=${CLUSTER_DNS_DOMAIN}'"
            sudo chmod 644 /etc/systemd/system/kubelet.service.d/20-kubelet.conf
            sudo systemctl enable --now kubelet
          # end configure  kubelet systemd

          # configure_kubeadm
          sudo sysctl -w net.bridge.bridge-nf-call-iptables=1
          sudo sysctl -w net.bridge.bridge-nf-call-ip6tables=1
          sudo sysctl -p

          if [[ $(systemctl is-active firewalld.service) == "active" ]]; then
             sudo systemctl disable --now firewalld
          fi

          # configure kubeadm yaml
          sudo cp /dev/stdin /etc/kubernetes/kubeadm_config.yaml <<< "---
          apiVersion: kubeadm.k8s.io/v1alpha1
          kind: MasterConfiguration
          api:
            advertiseAddress: ${MASTER_IP}
            bindPort: 443
          etcd:
            local:
              dataDir: /var/lib/etcd
              image:
          kubernetesVersion: v${CONTROL_PLANE_VERSION}
          token:
          kubeProxy:
            config:
              clusterCIDR: ${POD_CIDR}
          networking:
            dnsDomain: ${CLUSTER_DNS_DOMAIN}
            podSubnet: ${POD_CIDR}
            serviceSubnet: ${SERVICE_CIDR}
          "

          # YAML is whitespace picky. So, need to fix kubeadm_config
          sudo sed -r -i 's#^[[:blank:]]{2}##' /etc/kubernetes/kubeadm_config.yaml

          # Create and set bridge-nf-call-iptables to 1 to pass the kubeadm preflight check.
          # Workaround was found here:
          # http://zeeshanali.com/sysadmin/fixed-sysctl-cannot-stat-procsysnetbridgebridge-nf-call-iptables/
          if [[ $(sudo lsmod | grep br_netfilter -c) == 0 ]];then
            sudo modprobe br_netfilter
          fi

          # Allowing swap may not be reliable:
          # https://github.com/kubernetes/kubernetes/issues/53533
          sudo swapoff -a
          # end configure_kubeadm

          # run_kubeadm_master
          if ! sudo kubeadm init --config /etc/kubernetes/kubeadm_config.yaml; then
            echo >&2 "Unable to start kubeadm."
            exit 45
          fi

          for (( i = 0; i < 60; i++ )); do
            sudo kubectl --kubeconfig /etc/kubernetes/kubelet.conf annotate --overwrite node "$(hostname)" machine="${MACHINE}" && break
            sleep 1
          done

          # By default, use flannel for container network plugin, should make this configurable.
          if ping -c 1 raw.githubusercontent.com >/dev/null 2>&1; then
            sudo kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
          else
            sudo cp /dev/stdin /tmp/kube-flannel-v0.10.0.yaml <<< '
            ---
            kind: ClusterRole
            apiVersion: rbac.authorization.k8s.io/v1beta1
            metadata:
              name: flannel
            rules:
              - apiGroups:
                  - ""
                resources:
                  - pods
                verbs:
                  - get
              - apiGroups:
                  - ""
                resources:
                  - nodes
                verbs:
                  - list
                  - watch
              - apiGroups:
                  - ""
                resources:
                  - nodes/status
                verbs:
                  - patch
            ---
            kind: ClusterRoleBinding
            apiVersion: rbac.authorization.k8s.io/v1beta1
            metadata:
              name: flannel
            roleRef:
              apiGroup: rbac.authorization.k8s.io
              kind: ClusterRole
              name: flannel
            subjects:
            - kind: ServiceAccount
              name: flannel
              namespace: kube-system
            ---
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: flannel
              namespace: kube-system
            ---
            kind: ConfigMap
            apiVersion: v1
            metadata:
              name: kube-flannel-cfg
              namespace: kube-system
              labels:
                tier: node
                app: flannel
            data:
              cni-conf.json: |
                {
                  "name": "cbr0",
                  "plugins": [
                    {
                      "type": "flannel",
                      "delegate": {
                        "hairpinMode": true,
                        "isDefaultGateway": true
                      }
                    },
                    {
                      "type": "portmap",
                      "capabilities": {
                        "portMappings": true
                      }
                    }
                  ]
                }
              net-conf.json: |
                {
                  "Network": "10.244.0.0/16",
                  "Backend": {
                    "Type": "vxlan"
                  }
                }
            ---
            apiVersion: extensions/v1beta1
            kind: DaemonSet
            metadata:
              name: kube-flannel-ds
              namespace: kube-system
              labels:
                tier: node
                app: flannel
            spec:
              template:
                metadata:
                  labels:
                    tier: node
                    app: flannel
                spec:
                  hostNetwork: true
                  nodeSelector:
                    beta.kubernetes.io/arch: amd64
                  tolerations:
                  - key: node-role.kubernetes.io/master
                    operator: Exists
                    effect: NoSchedule
                  serviceAccountName: flannel
                  initContainers:
                  - name: install-cni
                    image: quay.io/coreos/flannel:v0.10.0-amd64
                    command:
                    - cp
                    args:
                    - -f
                    - /etc/kube-flannel/cni-conf.json
                    - /etc/cni/net.d/10-flannel.conflist
                    volumeMounts:
                    - name: cni
                      mountPath: /etc/cni/net.d
                    - name: flannel-cfg
                      mountPath: /etc/kube-flannel/
                  containers:
                  - name: kube-flannel
                    image: quay.io/coreos/flannel:v0.10.0-amd64
                    command:
                    - /opt/bin/flanneld
                    args:
                    - --ip-masq
                    - --kube-subnet-mgr
                    resources:
                      requests:
                        cpu: "100m"
                        memory: "50Mi"
                      limits:
                        cpu: "100m"
                        memory: "50Mi"
                    securityContext:
                      privileged: true
                    env:
                    - name: POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                    volumeMounts:
                    - name: run
                      mountPath: /run
                    - name: flannel-cfg
                      mountPath: /etc/kube-flannel/
                  volumes:
                    - name: run
                      hostPath:
                        path: /run
                    - name: cni
                      hostPath:
                        path: /etc/cni/net.d
                    - name: flannel-cfg
                      configMap:
                        name: kube-flannel-cfg
            '

            sudo kubectl --kubeconfig /etc/kubernetes/admin.conf apply -f /tmp/kube-flannel-v0.10.0.yaml
          fi

          echo done.
          ) 2>&1 | sudo tee /var/log/startup.log
